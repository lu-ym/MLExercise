{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "# [Blitz Course to TensorIR](https://tvm.apache.org/docs/tutorial/tensor_ir_blitz_course.html#sphx-glr-download-tutorial-tensor-ir-blitz-course-py)\n",
        "**Author**: [Siyuan Feng](https://github.com/Hzfengsy)\n",
        "\n",
        "TensorIR is a domain specific language for deep learning programs serving two broad purposes:\n",
        "\n",
        "- An implementation for transforming and optimizing programs on various hardware backends.\n",
        "\n",
        "- An abstraction for automatic _tensorized_ program optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tvm\n",
        "from tvm.ir.module import IRModule\n",
        "from tvm.script import tir as T\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## IRModule\n",
        "An IRModule is the central data structure in TVM, which contains deep learning programs.\n",
        "It is the basic object of interest of IR transformation and model building.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/tlc-pack/web-data/main/images/design/tvm_life_of_irmodule.png\" align=\"center\" width=\"50%\">\n",
        "\n",
        "This is the life cycle of an IRModule, which can be created from TVMScript. TensorIR schedule\n",
        "primitives and passes are two major ways to transform an IRModule. Also, a sequence of\n",
        "transformations on an IRModule is acceptable. Note that we can print an IRModule at **ANY** stage\n",
        "to TVMScript. After all transformations and optimizations are complete, we can build the IRModule\n",
        "to a runnable module to deploy on target devices.\n",
        "\n",
        "Based on the design of TensorIR and IRModule, we are able to create a new programming method:\n",
        "\n",
        "1. Write a program by TVMScript in a python-AST based syntax.\n",
        "\n",
        "2. Transform and optimize a program with python api.\n",
        "\n",
        "3. Interactively inspect and try the performance with an imperative style transformation API.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create an IRModule\n",
        "IRModule can be created by writing TVMScript, which is a round-trippable syntax for TVM IR.\n",
        "\n",
        "Different than creating a computational expression by Tensor Expression\n",
        "(`tutorial-tensor-expr-get-started`), TensorIR allow users to program through TVMScript,\n",
        "a language embedded in python AST. The new method makes it possible to write complex programs\n",
        "and further schedule and optimize it.\n",
        "\n",
        "Following is a simple example for vector addition.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "<module '__main__'> is a built-in module",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/data/lym/repoG/MLExercise/tvm_ir/tensor_ir_blitz_course.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bym/data/lym/repoG/MLExercise/tvm_ir/tensor_ir_blitz_course.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m@tvm\u001b[39m\u001b[39m.\u001b[39mscript\u001b[39m.\u001b[39mir_module\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bym/data/lym/repoG/MLExercise/tvm_ir/tensor_ir_blitz_course.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMyModule\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bym/data/lym/repoG/MLExercise/tvm_ir/tensor_ir_blitz_course.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m@T\u001b[39m\u001b[39m.\u001b[39mprim_func\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bym/data/lym/repoG/MLExercise/tvm_ir/tensor_ir_blitz_course.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m(a: T\u001b[39m.\u001b[39mhandle, b: T\u001b[39m.\u001b[39mhandle):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bym/data/lym/repoG/MLExercise/tvm_ir/tensor_ir_blitz_course.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m# We exchange data between function by handles, which are similar to pointer.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bym/data/lym/repoG/MLExercise/tvm_ir/tensor_ir_blitz_course.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         T\u001b[39m.\u001b[39mfunc_attr({\u001b[39m\"\u001b[39m\u001b[39mglobal_symbol\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmain\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtir.noalias\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m})\n",
            "\u001b[1;32m/data/lym/repoG/MLExercise/tvm_ir/tensor_ir_blitz_course.ipynb Cell 6\u001b[0m in \u001b[0;36mMyModule\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bym/data/lym/repoG/MLExercise/tvm_ir/tensor_ir_blitz_course.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m@tvm\u001b[39m\u001b[39m.\u001b[39mscript\u001b[39m.\u001b[39mir_module\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bym/data/lym/repoG/MLExercise/tvm_ir/tensor_ir_blitz_course.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMyModule\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bym/data/lym/repoG/MLExercise/tvm_ir/tensor_ir_blitz_course.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m@T\u001b[39;49m\u001b[39m.\u001b[39;49mprim_func\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bym/data/lym/repoG/MLExercise/tvm_ir/tensor_ir_blitz_course.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mdef\u001b[39;49;00m \u001b[39mmain\u001b[39;49m(a: T\u001b[39m.\u001b[39;49mhandle, b: T\u001b[39m.\u001b[39;49mhandle):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bym/data/lym/repoG/MLExercise/tvm_ir/tensor_ir_blitz_course.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m# We exchange data between function by handles, which are similar to pointer.\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bym/data/lym/repoG/MLExercise/tvm_ir/tensor_ir_blitz_course.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         T\u001b[39m.\u001b[39;49mfunc_attr({\u001b[39m\"\u001b[39;49m\u001b[39mglobal_symbol\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mmain\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtir.noalias\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mTrue\u001b[39;49;00m})\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bym/data/lym/repoG/MLExercise/tvm_ir/tensor_ir_blitz_course.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m# Create buffer from handles.\u001b[39;49;00m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tvm/script/tir/prim_func.py:40\u001b[0m, in \u001b[0;36mprim_func\u001b[0;34m(input_func)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39m\"\"\"Decorate a python function as tvm script.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39m    The result functions.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39mif\u001b[39;00m inspect\u001b[39m.\u001b[39misfunction(input_func):\n\u001b[0;32m---> 40\u001b[0m     result \u001b[39m=\u001b[39m from_source(input_func)\n\u001b[1;32m     41\u001b[0m     result\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m=\u001b[39m input_func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m     42\u001b[0m     result\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m \u001b[39m=\u001b[39m input_func\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tvm/script/parser.py:1098\u001b[0m, in \u001b[0;36mfrom_source\u001b[0;34m(input_func, tir_prefix)\u001b[0m\n\u001b[1;32m   1096\u001b[0m     namespace \u001b[39m=\u001b[39m [key \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m env\u001b[39m.\u001b[39mkeys() \u001b[39mif\u001b[39;00m env[key] \u001b[39m==\u001b[39m tir]\n\u001b[1;32m   1097\u001b[0m     parser \u001b[39m=\u001b[39m TVMScriptParser(start_line, namespace)\n\u001b[0;32m-> 1098\u001b[0m     result \u001b[39m=\u001b[39m to_ast(input_func, TVMDiagnosticCtx(), parser)\n\u001b[1;32m   1099\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m   1100\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/synr/compiler.py:722\u001b[0m, in \u001b[0;36mto_ast\u001b[0;34m(program, diagnostic_ctx, transformer)\u001b[0m\n\u001b[1;32m    720\u001b[0m mod \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39mgetmodule(program)\n\u001b[1;32m    721\u001b[0m \u001b[39mif\u001b[39;00m mod \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     full_source \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39;49mgetsource(mod)\n\u001b[1;32m    723\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    724\u001b[0m     full_source \u001b[39m=\u001b[39m source\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/inspect.py:1147\u001b[0m, in \u001b[0;36mgetsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetsource\u001b[39m(\u001b[39mobject\u001b[39m):\n\u001b[1;32m   1142\u001b[0m     \u001b[39m\"\"\"Return the text of the source code for an object.\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \n\u001b[1;32m   1144\u001b[0m \u001b[39m    The argument may be a module, class, method, function, traceback, frame,\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[39m    or code object.  The source code is returned as a single string.  An\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m \u001b[39m    OSError is raised if the source code cannot be retrieved.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1147\u001b[0m     lines, lnum \u001b[39m=\u001b[39m getsourcelines(\u001b[39mobject\u001b[39;49m)\n\u001b[1;32m   1148\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(lines)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/inspect.py:1129\u001b[0m, in \u001b[0;36mgetsourcelines\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[39m\"\"\"Return a list of source lines and starting line number for an object.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \n\u001b[1;32m   1123\u001b[0m \u001b[39mThe argument may be a module, class, method, function, traceback, frame,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39moriginal source file the first line of code was found.  An OSError is\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39mraised if the source code cannot be retrieved.\"\"\"\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mobject\u001b[39m \u001b[39m=\u001b[39m unwrap(\u001b[39mobject\u001b[39m)\n\u001b[0;32m-> 1129\u001b[0m lines, lnum \u001b[39m=\u001b[39m findsource(\u001b[39mobject\u001b[39;49m)\n\u001b[1;32m   1131\u001b[0m \u001b[39mif\u001b[39;00m istraceback(\u001b[39mobject\u001b[39m):\n\u001b[1;32m   1132\u001b[0m     \u001b[39mobject\u001b[39m \u001b[39m=\u001b[39m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39mtb_frame\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/inspect.py:940\u001b[0m, in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfindsource\u001b[39m(\u001b[39mobject\u001b[39m):\n\u001b[1;32m    933\u001b[0m     \u001b[39m\"\"\"Return the entire source file and starting line number for an object.\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \n\u001b[1;32m    935\u001b[0m \u001b[39m    The argument may be a module, class, method, function, traceback, frame,\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[39m    or code object.  The source code is returned as a list of all the lines\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[39m    in the file and the line number indexes a line in that list.  An OSError\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m    is raised if the source code cannot be retrieved.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 940\u001b[0m     file \u001b[39m=\u001b[39m getsourcefile(\u001b[39mobject\u001b[39;49m)\n\u001b[1;32m    941\u001b[0m     \u001b[39mif\u001b[39;00m file:\n\u001b[1;32m    942\u001b[0m         \u001b[39m# Invalidate cache if needed.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m         linecache\u001b[39m.\u001b[39mcheckcache(file)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/inspect.py:817\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetsourcefile\u001b[39m(\u001b[39mobject\u001b[39m):\n\u001b[1;32m    814\u001b[0m     \u001b[39m\"\"\"Return the filename that can be used to locate an object's source.\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[39m    Return None if no way can be identified to get the source.\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 817\u001b[0m     filename \u001b[39m=\u001b[39m getfile(\u001b[39mobject\u001b[39;49m)\n\u001b[1;32m    818\u001b[0m     all_bytecode_suffixes \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mmachinery\u001b[39m.\u001b[39mDEBUG_BYTECODE_SUFFIXES[:]\n\u001b[1;32m    819\u001b[0m     all_bytecode_suffixes \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mmachinery\u001b[39m.\u001b[39mOPTIMIZED_BYTECODE_SUFFIXES[:]\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/inspect.py:778\u001b[0m, in \u001b[0;36mgetfile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mobject\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m__file__\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    777\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__file__\u001b[39m\n\u001b[0;32m--> 778\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m is a built-in module\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mobject\u001b[39m))\n\u001b[1;32m    779\u001b[0m \u001b[39mif\u001b[39;00m isclass(\u001b[39mobject\u001b[39m):\n\u001b[1;32m    780\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mobject\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m__module__\u001b[39m\u001b[39m'\u001b[39m):\n",
            "\u001b[0;31mTypeError\u001b[0m: <module '__main__'> is a built-in module"
          ]
        }
      ],
      "source": [
        "@tvm.script.ir_module\n",
        "class MyModule:\n",
        "    @T.prim_func\n",
        "    def main(a: T.handle, b: T.handle):\n",
        "        # We exchange data between function by handles, which are similar to pointer.\n",
        "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": True})\n",
        "        # Create buffer from handles.\n",
        "        A = T.match_buffer(a, (8,), dtype=\"float32\")\n",
        "        B = T.match_buffer(b, (8,), dtype=\"float32\")\n",
        "        for i in range(8):\n",
        "            # A block is an abstraction for computation.\n",
        "            with T.block(\"B\"):\n",
        "                # Define a spatial block iterator and bind it to value i.\n",
        "                vi = T.axis.spatial(8, i)\n",
        "                B[vi] = A[vi] + 1.0\n",
        "\n",
        "\n",
        "ir_module = MyModule\n",
        "print(type(ir_module))\n",
        "print(ir_module.script())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Besides, we can also use tensor expression DSL to write simple operators, and convert them\n",
        "to an IRModule.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# from tvm.script import tir as T\n",
            "@tvm.script.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(A: T.Buffer[8, \"float32\"], B: T.Buffer[8, \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0 in T.serial(8):\n",
            "            with T.block(\"B\"):\n",
            "                i0_1 = T.axis.spatial(8, i0)\n",
            "                T.reads(A[i0_1])\n",
            "                T.writes(B[i0_1])\n",
            "                B[i0_1] = A[i0_1] + T.float32(1)\n",
            "    \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from tvm import te\n",
        "\n",
        "A = te.placeholder((8,), dtype=\"float32\", name=\"A\")\n",
        "B = te.compute((8,), lambda *i: A(*i) + 1.0, name=\"B\")\n",
        "func = te.create_prim_func([A, B])\n",
        "ir_module_from_te = IRModule({\"main\": func})\n",
        "print(ir_module_from_te.script())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build and Run an IRModule\n",
        "We can build the IRModule into a runnable module with specific target backends.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'tvm.driver.build_module.OperatorModule'>\n"
          ]
        }
      ],
      "source": [
        "mod = tvm.build(ir_module, target=\"llvm\")  # The module for CPU backends.\n",
        "print(type(mod))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare the input array and output array, then run the module.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0. 1. 2. 3. 4. 5. 6. 7.]\n",
            "[1. 2. 3. 4. 5. 6. 7. 8.]\n"
          ]
        }
      ],
      "source": [
        "a = tvm.nd.array(np.arange(8).astype(\"float32\"))\n",
        "b = tvm.nd.array(np.zeros((8,)).astype(\"float32\"))\n",
        "mod(a, b)\n",
        "print(a)\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transform an IRModule\n",
        "The IRModule is the central data structure for program optimization, which can be transformed\n",
        "by :code:`Schedule`.\n",
        "A schedule contains multiple primitive methods to interactively transform the program.\n",
        "Each primitive transforms the program in certain ways to bring additional performance optimizations.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/tlc-pack/web-data/main/images/design/tvm_tensor_ir_opt_flow.png\" align=\"center\" width=\"100%\">\n",
        "\n",
        "The image above is a typical workflow for optimizing a tensor program. First, we need to create a\n",
        "schedule on the initial IRModule created from either TVMScript or Tensor Expression. Then, a\n",
        "sequence of schedule primitives will help to improve the performance. And at last, we can lower\n",
        "and build it into a runnable module.\n",
        "\n",
        "Here we just demonstrate a very simple transformation. First we create schedule on the input `ir_module`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'tvm.tir.schedule.schedule.Schedule'>\n"
          ]
        }
      ],
      "source": [
        "sch = tvm.tir.Schedule(ir_module)\n",
        "print(type(sch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tile the loop into 3 loops and print the result.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# from tvm.script import tir as T\n",
            "@tvm.script.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(A: T.Buffer[8, \"float32\"], B: T.Buffer[8, \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i_0, i_1, i_2 in T.grid(2, 2, 2):\n",
            "            with T.block(\"B\"):\n",
            "                vi = T.axis.spatial(8, i_0 * 4 + i_1 * 2 + i_2)\n",
            "                T.reads(A[vi])\n",
            "                T.writes(B[vi])\n",
            "                B[vi] = A[vi] + T.float32(1)\n",
            "    \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get block by its name\n",
        "block_b = sch.get_block(\"B\")\n",
        "# Get loops surrounding the block\n",
        "(i,) = sch.get_loops(block_b)\n",
        "# Tile the loop nesting.\n",
        "i_0, i_1, i_2 = sch.split(i, factors=[2, 2, 2])\n",
        "print(sch.mod.script())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also reorder the loops. Now we move loop `i_2` to outside of `i_1`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# from tvm.script import tir as T\n",
            "@tvm.script.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(A: T.Buffer[8, \"float32\"], B: T.Buffer[8, \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i_0, i_2, i_1 in T.grid(2, 2, 2):\n",
            "            with T.block(\"B\"):\n",
            "                vi = T.axis.spatial(8, i_0 * 4 + i_1 * 2 + i_2)\n",
            "                T.reads(A[vi])\n",
            "                T.writes(B[vi])\n",
            "                B[vi] = A[vi] + T.float32(1)\n",
            "    \n",
            "\n"
          ]
        }
      ],
      "source": [
        "sch.reorder(i_0, i_2, i_1)\n",
        "print(sch.mod.script())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transform to a GPU program\n",
        "If we want to deploy models on GPUs, thread binding is necessary. Fortunately, we can\n",
        "also use primitives and do incrementally transformation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# from tvm.script import tir as T\n",
            "@tvm.script.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(A: T.Buffer[8, \"float32\"], B: T.Buffer[8, \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i_0 in T.thread_binding(2, thread=\"blockIdx.x\"):\n",
            "            for i_2 in T.thread_binding(2, thread=\"threadIdx.x\"):\n",
            "                for i_1 in T.serial(2):\n",
            "                    with T.block(\"B\"):\n",
            "                        vi = T.axis.spatial(8, i_0 * 4 + i_1 * 2 + i_2)\n",
            "                        T.reads(A[vi])\n",
            "                        T.writes(B[vi])\n",
            "                        B[vi] = A[vi] + T.float32(1)\n",
            "    \n",
            "\n"
          ]
        }
      ],
      "source": [
        "sch.bind(i_0, \"blockIdx.x\")\n",
        "sch.bind(i_2, \"threadIdx.x\")\n",
        "print(sch.mod.script())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After binding the threads, now build the IRModule with :code:`cuda` backends.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[16:39:16] /workspace/tvm/src/target/target_kind.cc:163: Warning: Unable to detect CUDA version, default to \"-arch=sm_20\" instead\n"
          ]
        },
        {
          "ename": "TVMError",
          "evalue": "Traceback (most recent call last):\n  4: TVMFuncCall\n  3: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS0_6ModuleERKNS0_3MapINS_6TargetENS_8IRModuleEvvEES7_EE17AssignTypedLambdaINS_L10__mk_TVM17MUlSB_S7_E_EEEvT_SsEUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SH_SL_\n  2: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)\n  1: tvm::codegen::Build(tvm::IRModule, tvm::Target)\n  0: _ZN3tvm7runtime6deta\n  File \"/workspace/tvm/src/target/codegen.cc\", line 58\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n  Check failed: (bf != nullptr) is false: target.build.cuda is not enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m/data/lym/repoG/MLExercise/tvm_ir/tensor_ir_blitz_course.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bym/data/lym/repoG/MLExercise/tvm_ir/tensor_ir_blitz_course.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m ctx \u001b[39m=\u001b[39m tvm\u001b[39m.\u001b[39mcuda(\u001b[39m0\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bym/data/lym/repoG/MLExercise/tvm_ir/tensor_ir_blitz_course.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m cuda_mod \u001b[39m=\u001b[39m tvm\u001b[39m.\u001b[39;49mbuild(sch\u001b[39m.\u001b[39;49mmod, target\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bym/data/lym/repoG/MLExercise/tvm_ir/tensor_ir_blitz_course.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m cuda_a \u001b[39m=\u001b[39m tvm\u001b[39m.\u001b[39mnd\u001b[39m.\u001b[39marray(np\u001b[39m.\u001b[39marange(\u001b[39m8\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m), ctx)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bym/data/lym/repoG/MLExercise/tvm_ir/tensor_ir_blitz_course.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m cuda_b \u001b[39m=\u001b[39m tvm\u001b[39m.\u001b[39mnd\u001b[39m.\u001b[39marray(np\u001b[39m.\u001b[39mzeros((\u001b[39m8\u001b[39m,))\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m), ctx)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/tvm/driver/build_module.py:282\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(inputs, args, target, target_host, runtime, name, binds)\u001b[0m\n\u001b[1;32m    278\u001b[0m     target_host \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mllvm\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m tvm\u001b[39m.\u001b[39mruntime\u001b[39m.\u001b[39menabled(\u001b[39m\"\u001b[39m\u001b[39mllvm\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstackvm\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m annotated_mods, target_host \u001b[39m=\u001b[39m Target\u001b[39m.\u001b[39mcanon_target_map_and_host(annotated_mods, target_host)\n\u001b[0;32m--> 282\u001b[0m rt_mod_host \u001b[39m=\u001b[39m _driver_ffi\u001b[39m.\u001b[39;49mtir_to_runtime(annotated_mods, target_host)\n\u001b[1;32m    284\u001b[0m annotated_mods, target_host \u001b[39m=\u001b[39m Target\u001b[39m.\u001b[39mcanon_target_map_and_host(annotated_mods, target_host)\n\u001b[1;32m    286\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(target_host, Target):\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/tvm/_ffi/_ctypes/packed_func.py:237\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    225\u001b[0m ret_tcode \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m    226\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    227\u001b[0m     _LIB\u001b[39m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    228\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    236\u001b[0m ):\n\u001b[0;32m--> 237\u001b[0m     \u001b[39mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    238\u001b[0m _ \u001b[39m=\u001b[39m temp_args\n\u001b[1;32m    239\u001b[0m _ \u001b[39m=\u001b[39m args\n",
            "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  4: TVMFuncCall\n  3: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS0_6ModuleERKNS0_3MapINS_6TargetENS_8IRModuleEvvEES7_EE17AssignTypedLambdaINS_L10__mk_TVM17MUlSB_S7_E_EEEvT_SsEUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SH_SL_\n  2: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)\n  1: tvm::codegen::Build(tvm::IRModule, tvm::Target)\n  0: _ZN3tvm7runtime6deta\n  File \"/workspace/tvm/src/target/codegen.cc\", line 58\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n  Check failed: (bf != nullptr) is false: target.build.cuda is not enabled"
          ]
        }
      ],
      "source": [
        "ctx = tvm.cuda(0)\n",
        "cuda_mod = tvm.build(sch.mod, target=\"cuda\")\n",
        "cuda_a = tvm.nd.array(np.arange(8).astype(\"float32\"), ctx)\n",
        "cuda_b = tvm.nd.array(np.zeros((8,)).astype(\"float32\"), ctx)\n",
        "cuda_mod(cuda_a, cuda_b)\n",
        "print(cuda_a)\n",
        "print(cuda_b)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7be21be0778d258b2a85d448d175280931934e7c4b8868294cf4ba82ea68406"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
